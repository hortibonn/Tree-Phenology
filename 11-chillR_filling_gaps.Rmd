# Filling gaps in temperature records

```{r, echo=FALSE,message=FALSE,warning=FALSE}
require(tidyr)
require(reshape2)
require(dplyr)
require(kableExtra)
require(ggplot2)
```

## Learning goals for this lesson {-#goals_gaps}

- see why having gaps in records can be quite problematic
- learn about (too?) simple ways to fill gaps in daily temperature records
- learn how to use data from auxiliary weather stations to fill gaps in daily temperature records
- learn about a creative way to close gaps in hourly temperature records

## Gaps

There's a lot of weather data out there, but most of it isn't perfect. Lots of things can go wrong when recording temperature data, including malfunctioning equipment, flat batteries, power cuts or lack of maintenance. Data archiving, transmission and storage in a database presents the next hurdle. Many datasets therefore have gaps that can be very annoying when you're trying to model agroclimatic conditions. Many scientific methods don't deal very well with missing data. So we need ways of filling such gaps.

## Filling short gaps in daily records

Weather records may be largely complete, except for isolated incidences of missing daily minimum or maximum temperatures. In such cases, we may get away with simple linear interpolation. This means, we take the last recorded value before a gaps and the first after the gap and compute the average. We can use equivalent procedures for slightly longer gaps (e.g. 2-3 days), and possibly even a few more (even though we should now be starting to feel a bit uneasy about this). Obviously, the longer the gaps, the more dubious this practice becomes. In case gaps extend to several weeks or even months, linear interpolation, which simply connects the start and end points of the gap by a straight line, may miss important features of the local temperature dynamics. In the extreme case, where we're missing an entire year of data, such linear interpolation would miss entire seasons, producing unacceptable errors. Yet `chillR` has a function to implement such simple interpolation - `interpolate_gaps()`.

```{r interpolate_gaps}
library(chillR)
library(tidyverse)
weather <- KA_weather %>% make_all_day_table()

Tmin_int <- interpolate_gaps(weather[,"Tmin"])

weather <- weather %>% mutate(Tmin = Tmin_int$interp,
                              Tmin_interpolated = Tmin_int$missing)

Tmax_int <- interpolate_gaps(weather[,"Tmax"])

weather <- weather %>% mutate(Tmax = Tmax_int$interp,
                              Tmax_interpolated = Tmax_int$missing)

```

The `fix_weather()` function in `chillR` uses the same procedure. If you run this without any additional arguments (just `fix_weather(weather)`), this function assumes you're trying to linearly interpolate all gaps in the `Tmin` and `Tmax` columns. If entire days are missing in the records, new lines are added for these days (using the `make_all_day_table()` function). You can also specify the range of years to apply this procedure for (by specifying `start_year` and `end_year`), the dates to work on (`start_date` and `end_date` - these are in Julian days, i.e. days of the year) and the names of columns you want to work on (needed if these are different from `Tmin` and `Tmax`)

```{r fix_weather}
# add an extra day to the KA_weather dataset that is not connected to the days that are already there.
# this creates a large gap, which we can then interpolate
KA_weather_gap <- rbind(KA_weather, c(Year = 2011,
                                      Month = 3,
                                      Day = 3,
                                      Tmax = 26,
                                      Tmin = 14)) 

# fill in the gaps between Julian date 300 (late October) and 100 (early April), only returning data between 2000 and 2011
fixed_winter_days <- KA_weather_gap %>% fix_weather(start_year = 2000,
                                                    end_year = 2011,
                                                    start_date = 300,
                                                    end_date = 100)

# fill in all gaps
fixed_all_days <- KA_weather_gap %>% fix_weather()

```

The `fix_weather()` function produces a list with two outputs:

- `weather`: a `data.frame` with the interpolated weather data, now including columns `no_Tmin` and `no_Tmax`, which contain `TRUE` for rows where the respective values were originally missing (`FALSE` otherwise).

- `QC`: a quality control object that summarizes how many values were interpolated for each season.

Here's how the QC elements look for the two interpolated datasets we just created:

```{r, eval = FALSE}
fixed_winter_days$QC
```

```{r, echo = FALSE}
kable(fixed_winter_days$QC, caption="Quality control summary produced by *fix_weather()*, with only winter days interpolated") %>%
  kable_styling("striped", position = "left", font_size = 10)

```

```{r, eval = FALSE}
fixed_all_days$QC
```

```{r, echo = FALSE}

kable(fixed_all_days$QC, caption="Quality control summary produced by *fix_weather()*, with all days interpolated") %>%
  kable_styling("striped", position = "left", font_size = 10)

```

As indicated above, linear interpolation is usually acceptable for short gaps in the records, but it gets increasingly less convincing, when the gaps are longer. Here's a quick demonstration of this:

```{r widening_gaps}

gap_weather <- KA_weather[200:305,]
gap_weather[,"Tmin_observed"] <- gap_weather$Tmin
gap_weather$Tmin[c(2,4:5,7:9,11:14,16:20,22:27,29:35,
                   37:44,46:54,56:65,67:77,79:90,92:104)] <- NA
fixed_gaps <- fix_weather(gap_weather)$weather

ggplot(data=fixed_gaps,
       aes(DATE,Tmin_observed)) +
  geom_line(lwd=1.3) +
  xlab("Date") +
  ylab("Daily minimum temperature (°C)") +
  geom_line(data=fixed_gaps,aes(DATE,Tmin),col="red",lwd=1.3)
```

The plot above shows the originally measured values in black and the interpolated values in red. To create gaps in the record for interpolation, I removed values from the original dataset, starting with gaps of length==1 on the left, and ending up with 13 missing values on the right. As you see, temperature dynamics are captured quite well on the left, but no longer very well on the right. Here's how far off we are:

```{r gaps_error}
fixed_gaps[,"error"] <- abs(fixed_gaps$Tmin - fixed_gaps$Tmin_observed)

ggplot(data=fixed_gaps,
       aes(DATE,error)) +
  geom_line(lwd=1.3) +
  xlab("Date") +
  ylab("Error introduced by interpolation (°C)") +
  geom_point(data=fixed_gaps[which(!fixed_gaps$no_Tmin),],
             aes(DATE,error),col="red",cex=3)

```

How big the errors get obviously depends on the nature of the dataset - was there a lot of variation during the period of interest, or not? Yet we see a tendency here of errors getting bigger and bigger with increasing gap sizes. Of course for the non-gap points (shown in red in the plot), the error is zero, but especially in the middle of the larger gaps, we can be quite far off from what actually happened.

For large gaps, we need a better procedure!

## Filling long gaps in daily records

Long gaps in temperature records are a problem - and we can't make this go away entirely. To stress the obvious again: We can't know exactly what the temperatures were in a given location at a particular time, if nobody measured them. But we can make pretty good guesses. For short gaps, linear interpolation may be enough. We could now also experiment with more complex interpolation algorithms, but this is not the direction I want to take here. Instead, let's try to look for additional data that can help us deal with this challenge.

We've already learned about one good source of temperature data, and of course there are lots of other records out there. Chances are that, unless we're working way back in the past, or on a remote island, we can find another weather station that is close enough to be in a climatically comparable setting. Some researchers have identified such stations and then simply used the data from there to fill gaps for the site of interest. This (as far as I understand it) is the preferred way of dealing with gaps in records in the [CIMIS](https://cimis.water.ca.gov/) network in California (at least this used to be the default option on their website).

This may work ok, if the auxiliary station is in a *very* similar climatic setting, but it will already introduce an error if we're dealing with some elevation differences, or with other landscape features that have climatic effects (e.g. lakes, sea, forest). Such features, as well as topography, can modulate temperatures at a particular place in a way that makes them poor proxies of temperatures in another location.

`chillR` contains a function, `patch_weather()`, that can fill gaps in a weather record based on a list of weather datasets from auxiliary stations. This function can test for (some) biases, correct data for bias in mean temperatures, and use the result to fill gaps in the record for the location of interest.

Let's try to fill gaps in the dataset for Bonn that we downloaded in the [Getting temperature data] lesson.

```{r}
Bonn <- read.csv("data/Bonn_chillR_weather.csv")

Bonn_QC <- fix_weather(Bonn)$QC

```

```{r, eval=FALSE}
Bonn_QC

```

```{r, echo=FALSE}
kable(Bonn_QC, caption="Quality control summary produced by *fix_weather()*") %>%
  kable_styling("striped", position = "left", font_size = 10)

```
As you see, this dataset has substantial gaps between 1998 and 2004 and in 2008 (almost all values missing), and some shorter gaps in 2015, 2018 and 2020.

We'll now need data from other weather stations in the neighborhood. To find them, we can again use the `handle_gsod()` function:
```{r find_close_stations, eval=FALSE}
station_list <- handle_gsod(action="list_stations",
                            location=c(7.10,50.73),
                            time_interval=c(1990,2020))
```

```{r load_station_list, echo=FALSE, message=FALSE}
station_list <- read.csv("data/station_list.csv")
```
```{r, eval=FALSE}
station_list

```

```{r, echo=FALSE}
kable(station_list, caption="List of GSOD weather stations close to Bonn") %>%
  kable_styling("striped", position = "left", font_size = 10)

```
We can see that many of the listed stations aren't very useful for us, because they only overlap with the record we already have for a few years, in some cases not at all. So it's quite possible that none of these stations can help us fill all the gaps in the temperature data for Bonn. But maybe we can combine data from multiple auxiliary stations to close all the gaps. Stations that look promising are BONN-HARDTHOEHE, BONN-ROLEBER, and NORVENICH. So let's download these and store them in a list. 

As of `chillR` version 0.74, the `handle_gsod` function can download multiple files at ones, returning a named list of station records. Let's use this function to download the records for the stations of interest (positions 2, 3 and 6 in the `station_list`).

```{r download_patch_list, eval=FALSE}

patch_weather<-
      handle_gsod(action = "download_weather",
                  location = as.character(station_list$chillR_code[c(2,3,6)]),
                  time_interval = c(1990,2020)) %>%
  handle_gsod()
```

```{r, eval=FALSE, echo=FALSE}

save_temperature_scenarios(patch_weather,
                           "data/",
                           "patch_weather")
```


```{r load_patch_weather, echo=FALSE, message=FALSE}
patch_weather<-load_temperature_scenarios("data/", "patch_weather")
```

Now we have a list of weather records that are potentially useful for filling gaps in our record for Bonn. We can now use the `patch_daily_temperatures()` function to implement this.

```{r}

patched <- patch_daily_temperatures(weather = Bonn,
                                    patch_weather = patch_weather)

```

You can take a look at what happened by looking at the `statistics` element of the `patched` object (call `patched$statistics`):

```{r, eval=FALSE}
patched$statistics[[1]]
```

```{r, echo=FALSE}
kable(patched$statistics[[1]],
      caption=paste("Patch statistics for",
                    names(patched$statistics)[1])) %>%
    kable_styling("striped", position = "left", font_size = 10)
```

```{r, eval=FALSE}
patched$statistics[[2]]
```

```{r, echo=FALSE}
kable(patched$statistics[[2]],
      caption=paste("Patch statistics for",
                    names(patched$statistics)[2])) %>%
    kable_styling("striped", position = "left", font_size = 10)
```

```{r, eval=FALSE}
patched$statistics[[3]]
```

```{r, echo=FALSE}
kable(patched$statistics[[3]],
      caption=paste("Patch statistics for",
                    names(patched$statistics)[3])) %>%
    kable_styling("striped", position = "left", font_size = 10)
```


Here we see an analysis of how similar the temperature records (separately for `Tmin` and `Tmax`) were between each auxiliary station and our station of interest in Bonn, based on days for which both stations had data. Besides the number of days for which information was taken from each auxiliary station (`filled`) and the number of gaps that remained afterwards (`gaps_remain`), we see two quality statistics:

1) the mean bias (`mean_bias`), i.e. the mean temperature difference.
2) the standard deviation of the daily differences (`stdev_bias`).

The `mean_bias` is easy to address by adding or subtracting the respective value, when we transfer daily temperature values from one station to the other. The `patch_daily_temperatures()` function does that for us automatically. We may still want to set some limit to how much we can accept here, but in principle, this can be addressed. What is more problematic is the `stdev_bias` metric. This basically indicates the extent of additional (possibly unsystematic) differences between stations. I don't know if this can be adjusted (I haven't figured it out yet), so for now we should treat this as an exclusion criterion (i.e. if `stdev_bias` is above a certain value, we reject the station). Let's set some limits for both metrics, which we can pass as arguments to `patch_daily_temperatures()` (using the `max_mean_bias` and `max_stdev_bias` parameters). Let's cap the `mean_bias` at 1 °C and the `stdev_bias` at 2°C, and then look at the statistics again.

```{r, results='asis'}
patched <- patch_daily_temperatures(weather = Bonn,
                                    patch_weather = patch_weather,
                                    max_mean_bias = 1,
                                    max_stdev_bias = 2)
```

```{r, eval=FALSE}
patched$statistics[[1]]
```

```{r, echo=FALSE}
kable(patched$statistics[[1]],
      caption=paste("Patch statistics for",
                    names(patched$statistics)[1])) %>%
  kable_styling("striped", position = "left",font_size = 10)

```

```{r, eval=FALSE}
patched$statistics[[2]]
```

```{r, echo=FALSE}
kable(patched$statistics[[2]],
      caption=paste("Patch statistics for",
                    names(patched$statistics)[2])) %>%
  kable_styling("striped", position = "left",font_size = 10)

```

```{r, eval=FALSE}
patched$statistics[[3]]
```

```{r, echo=FALSE}
kable(patched$statistics[[3]],
      caption=paste("Patch statistics for",
                    names(patched$statistics)[3])) %>%
  kable_styling("striped", position = "left",font_size = 10)

```

We can see that all records from BONN-HARDTHOEHE, as well as the `Tmax` records from BONN-ROLEBER were rejected, because they didn't pass our `mean_bias` filter. Still, since the data from NORVENICH are pretty good, we were able to fill `r patched$statistics[[1]]$filled[1] + patched$statistics[[2]]$filled[1] + patched$statistics[[3]]$filled[1]` gaps for `Tmin` and `r patched$statistics[[1]]$filled[2] + patched$statistics[[2]]$filled[2] + patched$statistics[[3]]$filled[2]` for `Tmax`. Only `r patched$statistics[[3]]$gaps_remain[1]` and `r patched$statistics[[3]]$gaps_remain[2]` gaps remain for `Tmin` and `Tmax`, respectively.

Let's use the `fix_weather()` function to take a look at where the remaining gaps are:

```{r, eval=FALSE}
post_patch_stats <- fix_weather(patched)$QC

post_patch_stats
```



```{r, echo=FALSE}
post_patch_stats <- fix_weather(patched)$QC

kable(post_patch_stats,
      caption="Data completeness table for the weather record from Bonn, after applying the patch procedure") %>%
  kable_styling("striped", position = "left", font_size = 10)

```
We can see that we managed to fill almost all gaps, with only data for 1 day missing after the patching. It seems safe to use linear interpolation for such a short gap. The `fix_weather()` function can do this for us.

```{r interpolate_remaining_gaps}
Bonn_weather<-fix_weather(patched)

```

### Bias-correction for shorter intervals

In the `patch_daily_temperatures` function, the bias correction is based on the average difference between the temperatures of a pair of weather stations over the entire year. It is possible, however, that the between-station bias varies throughout the year. We may then find that a particular station is a useful data source for temperatures in certain seasons but not in others. If we allow for different bias corrections during different parts of the year, we may also find that we can get better approximations, i.e. smaller biases than if we look at the entire year.

The `patch_daily_temps` (not `...temperatures`) function allows us to do this. As a default, it evaluates temperature records on a monthly basis, i.e. it makes separate between-station comparisons for the temperatures of each calendar month. It can then take separate decisions on whether a potential auxiliary station is a useful proxy for temperatures in this monthly interval, and it can apply month-specific bias correction.


```{r}


patched_monthly <- patch_daily_temps(weather = Bonn,
                                     patch_weather = patch_weather,
                                     max_mean_bias = 1,
                                     max_stdev_bias = 2,
                                     time_interval = "month")

```

Here's the finding for minimum temperatures for the `r names(patched_monthly$statistics$Tmin)[[1]]` station.

```{r, eval=FALSE}
patched_monthly$statistics$Tmin$NORVENICH

```

```{r, echo=FALSE}
kable(patched_monthly$statistics$Tmin$NORVENICH,
      caption="Bias analysis table for the first station in the proxy station list, showing biases on a monthly level") %>%
  kable_styling("striped", position = "left", font_size = 10)

```

We now see that the mean bias really varies quite a bit, so that we probably benefit from a month-specific bias correction.

The `time_interval` parameter of the `patch_daily_temps` function allows us to specify the interval we want to use. Intervals can be `month` or `week` but also multiples of these, such as `10 days` or `2 weeks`. Note that the function will start counting these intervals on 1st January in each year. This may lead to intervals at the end of the year that are smaller than the interval you selected (this generates warnings, as you can see below). Note also that the smaller these intervals get, the less data can be used for determining the bias. Especially for short time series, a very short interval may therefore not be desirable.

```{r}
patched_2weeks <- patch_daily_temps(weather = Bonn,
                                    patch_weather = patch_weather,
                                    max_mean_bias = 1,
                                    max_stdev_bias = 2,
                                    time_interval = "2 weeks")
```

To illustrate the effects of this, let's create 5000 gaps in the Bonn weather record and fill them with proxy data using annual, monthly and bi-weekly intervals for the bias evaluation. We can plot the resulting errors with `ggplot2` using a violin plot.

```{r, warning=FALSE}

Gaps <- sample(seq(1:nrow(Bonn)), size = 5000, replace = FALSE)

Bonn_gaps <- Bonn %>% mutate(obs_Tmin=Tmin,
                             obs_Tmax=Tmax)
Bonn_gaps$Tmin[Gaps] <- NA
Bonn_gaps$Tmax[Gaps] <- NA

patch_annual <- patch_daily_temps(weather = Bonn_gaps,
                                  patch_weather = patch_weather,
                                  max_mean_bias = 1,
                                  max_stdev_bias = 2,
                                  time_interval = "year")
patch_month <- patch_daily_temps(weather = Bonn_gaps,
                                 patch_weather = patch_weather,
                                 max_mean_bias = 1,
                                 max_stdev_bias = 2,
                                 time_interval = "month")
patch_2weeks <- patch_daily_temps(weather = Bonn_gaps,
                                  patch_weather = patch_weather,
                                  max_mean_bias = 1,
                                  max_stdev_bias = 2,
                                  time_interval = "2 weeks")

Bonn_gaps[,"Tmin_annual"] <- Bonn_gaps$obs_Tmin - patch_annual$weather$Tmin
Bonn_gaps[,"Tmax_annual"] <- Bonn_gaps$obs_Tmax - patch_annual$weather$Tmax
Bonn_gaps[,"Tmin_month"] <- Bonn_gaps$obs_Tmin - patch_month$weather$Tmin
Bonn_gaps[,"Tmax_month"] <- Bonn_gaps$obs_Tmax - patch_month$weather$Tmax
Bonn_gaps[,"Tmin_2weeks"] <- Bonn_gaps$obs_Tmin - patch_2weeks$weather$Tmin
Bonn_gaps[,"Tmax_2weeks"] <- Bonn_gaps$obs_Tmax - patch_2weeks$weather$Tmax

Interval_eval <- Bonn_gaps %>%
  filter(is.na(Tmin)) %>%
  pivot_longer(Tmin_annual:Tmax_2weeks) %>%
  mutate(Type=factor(name,
                     levels = c("Tmin_annual",
                                "Tmin_month",
                                "Tmin_2weeks",
                                "Tmax_annual",
                                "Tmax_month",
                                "Tmax_2weeks")) )

ggplot(Interval_eval,
       aes(Type,value)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  xlab("Variable and bias evaluation interval") +
  ylab("Prediction error")

```

We can also evaluate the mean daily error.

```{r}

error_eval <-
  data.frame(Variable = c(rep("Tmin",3),rep("Tmax",3)),
             Interval = rep(c("Year","Month","Two weeks"),2),
             Error = c(
             mean(abs(Bonn_gaps$Tmin_annual[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmin_month[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmin_2weeks[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmax_annual[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmax_month[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmax_2weeks[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE))
             )


```
```{r, eval=FALSE}
error_eval

```

```{r, echo=FALSE}
kable(error_eval,
      caption="Mean absolute prediction error for minimum and maximum
      temperature when using different time intervals as a basis for correcting
      for between-station biases") %>%
  kable_styling("striped", position = "left", font_size = 10)

```


As cou can see, the improvement wasn't very impressive here, possibly because the stations we used are quite close to each other and the weather doesn't differ much between them.

### Saving the data for later

Now we should make sure that we save one of these files for later. I'll choose the dataset where we based the bias correction on monthly intervals. We still need to interpolate the one missing day, before we can save this.

```{r}

monthly_bias_fixed <- fix_weather(patched_monthly)
```

Now we have a fairly convincing long-term record (from 1990 to 2020) for Bonn, with no more gaps remaining. Let's save this now.

```{r save_Bonn_weather, eval=FALSE}
write.csv(monthly_bias_fixed$weather,
          "data/Bonn_weather.csv")
```
Note that I only saved the `weather` element. This is a data.frame that can easily be saved as a spreadsheet table (`.csv` file). We can also save lists (including the `QC` element in this case), but that's a bit more complicated, so let's save this for later.

## Filling gaps in hourly records

When it comes to gaps, hourly records are a lot harder to handle, because linear interpolation usually isn't an option. This works for very short gaps only, but it's probably quite obvious that for gaps that extend over multiple days, you can't just connect the start and end points with a straight line. This makes it very hard to make use of hourly records, even when they are available. Since I've come across this problem a few times, I thought of a procedure that can do this [@luedeling_interpolating_2018]. 

Sometimes we have actual records of hourly temperatures. While this is preferable, in principle, to deriving hourly data from daily records, it may cause problems when the record isn't complete. An example of such a record is the `Winters_hours_gaps` dataset contained in `chillR`. Such situations arise quite often, because temperature loggers can temporarily fail for many reasons. Let's first look at what happens if we simply use linear interpolation:

```{r hourly_linear,echo=FALSE}
Winters_hours_gaps[,"DATE"] <- ISOdate(Winters_hours_gaps$Year,
                                       Winters_hours_gaps$Month,
                                       Winters_hours_gaps$Day,
                                       Winters_hours_gaps$Hour)
Winters_hours_gaps[,"interpolated"] <-
  interpolate_gaps(Winters_hours_gaps$Temp_gaps)$interp

ggplot(data = Winters_hours_gaps[50:300,],
       aes(DATE,Temp)) +
  geom_line(lwd = 1.3) +
  ylab("Temperature (°C)") +
  xlab("Date") +
  geom_line(data = Winters_hours_gaps[50:300,],
            aes(DATE,interpolated),
            col = "red",lwd = 1.3) +
  theme_bw(base_size = 20)  

```

In this interpolation, some daytime or nighttime cycles were missed entirely, which can lead to substantial errors when calculating agroclimatic metrics, such as chill or heat stress, that are of particular concern during the warmest and coolest parts of the day.

`chillR`'s `interpolate_gaps_hourly()` function provides an algorithm that can produce credible and continuous hourly records from such a patchy dataset. It combines several of the elements described above, but also adds functionality to derive daily temperature extremes from hourly data that were recorded. Without going into too much detail, here is the rough mode of operation:

- Express temperatures for each hour as a function of daily temperature extremes using the functions of @linvill1990calculating. According to this idealized curve, all hourly temperatures can be expressed as a function of Tmin and Tmax on the previous, same or next day of the temperature record (depending on which hour is of interest). For a day with a complete record, 24 equations can be set up.

- For each daily temperature extreme, empirically solve the system of all equations that contain the respective Tmin or Tmax variable (this is only attempted, when a minimum of 5 equations are available, to avoid spurious results).

- Close gaps in the resulting dataset of daily Tmin and Tmax using data from proxy stations or, as a last resort, linear interpolation.

- Compute idealized temperature curves from the now continuous record of daily Tmin and Tmax values.

- Calculate the difference between recorded temperatures and this idealized curve.

- Linearly interpolate this difference and add then result to the idealized temperature curve.

The following code calls this function for the Winters dataset, using daily data from a nearby station of the California Irrigation Management Information System (CIMIS) as a proxy. This is retrieved with the `handle_cimis()` function, which works similarly to the `handle_gsod()` function described above. Note that the CIMIS database seems to have occasional connectivity problems, and they've at least once made changes to their data storage system that required changes to the `handle_cimis` function. So it's possible that the process times out or returns an error.

```{r get_Winters_daily, eval=FALSE}
stations <- handle_cimis("list_stations",
                         location = c(-122,38.5))
downloaded_winters <- handle_cimis("download_weather",
                                   stations$chillR_code[2],
                                   time_interval = c(2008,2008))
winters_daily <- handle_cimis(downloaded_winters)$weather

```
Here's what the dataset looks like:

```{r echo=FALSE}
winters_daily <- read.csv("data/winters_daily.csv")
knitr::kable(winters_daily[1:5,]) %>%
  kable_styling("striped", position = "left",font_size = 10)
```

And here is the call of the `interpolate_gaps_hourly()` function:

```{r}
to_interp <- Winters_hours_gaps
to_interp[,"Temp_recorded"] <- to_interp[,"Temp"]
to_interp[,"Temp"] <- to_interp[,"Temp_gaps"]
interp <- interpolate_gaps_hourly(hourtemps = to_interp,
                                  latitude = 38.5,
                                  daily_temps = list(Winters=winters_daily))

```

The resulting dataset has two elements: `$weather` and `daily_patch_report`. Let's first look at the `daily_patch_report` element:

```{r echo=FALSE}
knitr::kable(interp$daily_patch_report,row.names = FALSE, align = "r")  %>%
  kable_styling("striped", position = "left",font_size = 10)
```

This table contains information on how many gaps in the daily record were filled by solving the system of hourly equations ('solved'), how many Tmin and Tmax values were derived from proxy stations (listed by name, if names were provided in the call to `interpolate_gaps_hourly`; otherwise as station_x), and how many were filled by linear interpolation (this option can be turned off using the `interpolate_remaining` parameter). For proxy stations, it also provides the bias in mean Tmin and Tmax, which has been corrected, as well as the bias in the standard deviation of Tmin and Tmax (which was *not* corrected).

The `$weather` element of the interpolation result contains the table of interpolated temperatures.

```{r echo=FALSE,results='as.is'}
knitr::kable(interp$weather[30:45,c(1:4,7,10)], row.names = FALSE,
             align=c("r","r","r","r","r","r"))  %>%
  kable_styling("striped", font_size = 10)
```

Here's a plot of part of the data:

```{r hourly_interpolation,echo=FALSE, warning=FALSE}
inter <- interp$weather
inter[,"DATE"] <- ISOdate(inter$Year,
                          inter$Month,
                          inter$Day,
                          inter$Hour)

ggplot(data = inter[50:300,],
       aes(DATE,Temp_recorded)) +
  geom_line(lwd = 1.3,
            col = "gray") +
  ylab("Temperature (°C)") +
  xlab("Date") +
  geom_line(data = inter[50:300,],
            aes(DATE,Temp),
            lwd = 1.3,
            col = "red") +
  geom_line(data = inter[50:300,],
            aes(DATE,Temp_gaps),
            lwd = 1.3) +
  theme_bw(base_size = 20)

```

This illustration shows that the `interpolate_gaps_hourly()` function produced a pretty good approximation (red lines) to the actual temperatures (gray line).

### Accuracy assessment

Since the actual hourly temperatures are known, we can evaluate the accuracy of the predictions produced by the various interpolation methods. A common measure for validating predictions is the Root Mean Square Error of the Prediction (RMSEP):

$$RMSEP=\sqrt{\frac{\sum_{i=1}^n(\hat{y}_i-y_i)^2}{n}}$$, with $\hat{y}_i$ being the observed values, $y_i$ the predicted values, and $n$ the number of values. 

The RMSEP provides an indication of how far each predicted value deviates, on average, from the actual values. It is, however, quite difficult to interpret RMSEP values alone, because whether they indicate a good or poor model fit depends on how variable the actual values are. For instance, an RMSEP of 5 days for a phenology model (which is close to, but not quite the same as a mean error of 5 days), could indicate a very good model, if observed dates vary by several weeks or months (e.g. for bloom dates of deciduous trees), but a terrible model, if the phenological stage of interest occurs on the same day every year (e.g. the 'phenological' event of candles lighting up on 'festive indoor conifers').

This is why it makes sense to include in such accuracy assessment the variation in observed values. This can be achieved by dividing the standard deviation of the observed data by the RMSEP to calculate the Residual Prediction Deviation (RPD):

$$RPD=\frac{sd_y}{RMSEP}$$
This equation contains the RMSEP as computed above as well as the standard deviation of the observed values from their mean, defined as

$$sd_y=\sqrt{\frac{\sum_{i=1}^n(y_i-\bar{y})^2}{n-1}}$$

, with $\bar{y}$ being the mean over all observations.

The RPD is more useful than the RMSEP, but its use of the standard deviation can be a problem, when actual values of $y$ aren't normally distributed (then the standard deviation can be a poor measure of variation). A more robust approach is use the interquartile range instead of the standard deviation. This metric is called the Ratio of Performance to InterQuartile distance (RPIQ):

$$RPIQ=\frac{IQ}{RMSEP}$$

IQ is calculated by subtracting the 75^th^ percentile of the distribution of all $y$ from the 25^th^ percentile.

```{r}
require(stats)
y <- rnorm(100)
IQ <- quantile(y)[4] - quantile(y)[2]
```

The RPIQ score is a bit harder to evaluate than the RMSEP, with different quality thresholds in use and a very high context dependency. Quite commonly, values above 2 are considered 'good' or even 'excellent', though some studies use substantially higher thresholds (up to 8 for excellence).

Since the RPIQ makes no assumption about the distribution of $y$, let's use this for assessing the accuracy of the various interpolation methods. We have a total of four methods to evaluate:

* **idealized** temperature curves from **daily** records of Tmin and Tmax, based on records from a nearby weather station
* **idealized** temperature curves from **daily** records of Tmin and Tmax, based on records from **the same location**  
* **linear** interpolation of **hourly** temperatures
* interpolation of **hourly** temperatures with **interpolate_gaps_hourly**

For option 2, we first have to generate a dataset of daily minimum and maximum temperatures from the hourly records. We can do this with the `make_all_day_table()` function (see documentation for this function for details).

```{r}
inter <- interp$weather
inter[,"DATE"] <- ISOdate(inter$Year,
                          inter$Month,
                          inter$Day,
                          inter$Hour)

orchard_extremes <- make_all_day_table(inter,
                                       timestep = "day",
                                       input_timestep = "hour")
```

Let's first look at the performance of the four methods for the periods that were missing in the hourly temperature record:

```{r}
winters_hours <- stack_hourly_temps(fix_weather(winters_daily),
                                    latitude = 38)$hourtemps

start_hour_winters <- which(winters_hours$Year == head(inter$Year,1)&
                              winters_hours$Month == head(inter$Month,1)&
                              winters_hours$Day == head(inter$Day,1)&
                              winters_hours$Hour == head(inter$Hour,1))

end_hour_winters <- which(winters_hours$Year == tail(inter$Year,1)&
                            winters_hours$Month == tail(inter$Month,1)&
                            winters_hours$Day == tail(inter$Day,1)&
                            winters_hours$Hour == tail(inter$Hour,1))

orchard_hours <- stack_hourly_temps(orchard_extremes,
                                    latitude = 38)$hourtemps

start_hour_orchard <- which(orchard_hours$Year == head(inter$Year,1)&
                              orchard_hours$Month == head(inter$Month,1)&
                              orchard_hours$Day == head(inter$Day,1)&
                              orchard_hours$Hour == head(inter$Hour,1))

end_hour_orchard <- which(orchard_hours$Year == tail(inter$Year,1)&
                            orchard_hours$Month == tail(inter$Month,1)&
                            orchard_hours$Day == tail(inter$Day,1)&
                            orchard_hours$Hour == tail(inter$Hour,1))

observed <- inter$Temp_recorded
option1 <- winters_hours$Temp[start_hour_winters:end_hour_winters]
option2 <- orchard_hours$Temp[start_hour_orchard:end_hour_orchard]
option3 <- interpolate_gaps(inter$Temp_gaps)$interp
option4 <- inter$Temp

eval_table <-
  eval_table_gaps <-
  data.frame(Option = 1:4,
             Input_data = c("daily","daily","hourly","hourly"),
             Interpolation_method = c("from proxy","local extremes",
                                      "linear","hourly interpolation"),
             RMSEP = NA,
             RPIQ = NA)

observed_gaps <- observed[which(is.na(inter$Temp_gaps))]
option1_gaps <- option1[which(is.na(inter$Temp_gaps))]
option2_gaps <- option2[which(is.na(inter$Temp_gaps))]
option3_gaps <- option3[which(is.na(inter$Temp_gaps))]
option4_gaps <- option4[which(is.na(inter$Temp_gaps))]

eval_table_gaps[,"RMSEP"] <- round(c(RMSEP(option1_gaps, observed_gaps),
                                     RMSEP(option2_gaps, observed_gaps),
                                     RMSEP(option3_gaps, observed_gaps),
                                     RMSEP(option4_gaps, observed_gaps)),
                                   1)

eval_table_gaps[,"RPIQ"] <- round(c(RPIQ(option1_gaps, observed_gaps),
                                    RPIQ(option2_gaps, observed_gaps),
                                    RPIQ(option3_gaps, observed_gaps),
                                    RPIQ(option4_gaps, observed_gaps)),
                                  1)

```

```{r, eval=FALSE}
eval_table_gaps
```


```{r, echo=FALSE}
knitr::kable(eval_table_gaps,row.names = FALSE)  %>%
  kable_styling("striped", position = "left", font_size = 10)
```

This table shows that the `interpolate_gaps_hourly` function produced the best results, with an RMSEP of `r round(eval_table_gaps$RMSEP[4],1)` and an RPIQ of `r round(eval_table_gaps$RPIQ[4],1)`. It's interesting to note that option 3, where hourly records collected in the orchard were interpolated linearly, produced the worst fit. This highlights that, at least in this case, using an idealized temperature curve to close gaps in daily temperatures from the orchard (option 2) and even from the proxy station (option 1) produced more accurate results. Naturally, the quality of the latter approach will depend on the similarity between weather at the proxy station and in the orchard (in this case, this should be quite similar).

Restricting the comparison to only the gaps in the record is a bit unfair, because of course records produced by option 3 (linear interpolation of hourly records from the orchard) are completely accurate for hours, when temperatures were recorded. So let's also compare the relative performance of the four methods across all hours of the record.

```{r}
eval_table <-
  data.frame(Option = 1:4,
             Input_data = c("daily","daily","hourly","hourly"),
             Interpolation_method = c("from proxy","local extremes",
                                      "linear","hourly interpolation"),
             RMSEP = NA,
             RPIQ = NA)

eval_table[,"RMSEP"] <- round(c(RMSEP(option1, observed),
                                RMSEP(option2, observed),
                                RMSEP(option3, observed),
                                RMSEP(option4, observed)),
                              1)

eval_table[,"RPIQ"] <- round(c(RPIQ(option1, observed),
                               RPIQ(option2, observed),
                               RPIQ(option3, observed),
                               RPIQ(option4, observed)),
                             1)

```

```{r, eval=FALSE}
eval_table
```

```{r, echo=FALSE}
knitr::kable(eval_table,row.names = FALSE)  %>%
  kable_styling("striped", position = "left",font_size = 10)

```

The relative performance of the methods on the whole dataset is quite similar to the previous assessment. The quality of the proxy-based idealized temperature curves went down slightly, while all other approaches saw improvements in quality (lower RMSEP and higher RPIQ). The RPIQ values for the two interpolations that were based on local data (options 2 and 4) are very high, especially for option 4, which used the `interpolate_gaps_hourly` function. The RPIQ score for this option almost exceeds the 'excellence' threshold for the most conservative RPIQ evaluation scheme that I've come across (8). I find this quite remarkable, given the variable nature of daily temperature fluctuations and the fact that about half of the actually recorded values were removed before running the interpolation.

***In conclusion, the ```interpolate_gaps_hourly``` function provided a very good approximation of hourly temperatures for times when no values were recorded.***

### Computing agroclimatic metrics

Finally, let's look at the implication of the choice of interpolation method on chill and heat estimates. If we're interested in using the Dynamic Model for winter chill or the Growing Degree Hours model for heat, we can simply calculate this using the `Dynamic_Model` and `GDH()` functions in `chillR`. For more functionality, see the `chilling()` and particularly the `tempResponse()` functions.

Let's first look at the implications of method choice on chill accumulation:

```{r}
all_chill <- data.frame(DATE = inter$DATE,
                        "Obs" = Dynamic_Model(observed),
                        "Opt1" = Dynamic_Model(option1),
                        "Opt2" = Dynamic_Model(option2),
                        "Opt3" = Dynamic_Model(option3),
                        "Opt4" = Dynamic_Model(option4))

all_chill <- pivot_longer(all_chill, Obs:Opt4)

all_chill[which(all_chill$name == "Obs"),"Method"] <- 
  "Observed temperatures"
all_chill[which(all_chill$name == "Opt1"),"Method"] <- 
  "Option 1 - idealized record from proxy data"
all_chill[which(all_chill$name == "Opt2"),"Method"] <-
  "Option 2 - idealized record from daily orchard data"
all_chill[which(all_chill$name == "Opt3"),"Method"] <-
  "Option 3 - linear interpolation of hourly data"
all_chill[which(all_chill$name == "Opt4"),"Method"] <-
  "Option 4 - use of interpolate_gaps_hourly"

```

```{r chill_accumulation,echo=FALSE}

ggplot(data=all_chill,
       aes(DATE,
           value,
           colour=Method)) +
  geom_line(lwd = 1.3) +
  ylab("Chill accumulation (Chill Portions)") +
  xlab("Date") +
  theme_bw(base_size = 15) +
  theme(legend.position = c(0.4, 0.85))

```

This figure shows that chill accumulation differed substantially between the options. Both the use of proxy data and the use of linear interpolation of hourly temperatures led to considerable overestimation of chill accumulation. 

Here is the same assessment for heat:

```{r}

all_heat <- data.frame(DATE = inter$DATE,
                       "Obs" = GDH(observed),
                       "Opt1" = GDH(option1),
                       "Opt2" = GDH(option2),
                       "Opt3" = GDH(option3),
                       "Opt4" = GDH(option4))

all_heat <- pivot_longer(all_heat, Obs:Opt4)

all_heat[which(all_heat$name == "Obs"),"Method"] <-
  "Observed temperatures"
all_heat[which(all_heat$name == "Opt1"),"Method"] <-
  "Option 1 - idealized record from proxy data"
all_heat[which(all_heat$name == "Opt2"),"Method"] <-
  "Option 2 - idealized record from daily orchard data"
all_heat[which(all_heat$name == "Opt3"),"Method"] <-
  "Option 3 - linear interpolation of hourly data"
all_heat[which(all_heat$name == "Opt4"),"Method"] <-
  "Option 4 - use of interpolate_gaps_hourly"
```

```{r heat_accumulation,echo=FALSE}

ggplot(data = all_heat,
       aes(DATE,
           value,
           colour=Method)) +
  geom_line(lwd = 1.3) +
  ylab("Heat accumulation (Growing Degree Hours)") +
  xlab("Date") +
  theme_bw(base_size = 15) +
  theme(legend.position = c(0.4, 0.85))

```

This comparison doesn't look quite as bad as for chill accumulation, but also here, option 4 clearly provided the most accurate estimate (it almost coincides with the black line, making the difference hard to see).

This dataset didn't cover the winter season, so the chill numbers aren't too meaningful, but it is nevertheless instructive to compare the total accumulation of chill and heat over the whole temperature record:

```{r echo=FALSE,results='as.is'}

chill_heat_eval <- rbind(data.frame(Option = 0,
                                    Input_data = "observed",
                                    Interpolation_method = "none"),
                         eval_table[,1:3])

chill_heat_eval[,"Chill Portions"] <-
  round(c(max(all_chill$value[which(all_chill$name == "Obs")]),
          max(all_chill$value[which(all_chill$name == "Opt1")]),
          max(all_chill$value[which(all_chill$name == "Opt2")]),
          max(all_chill$value[which(all_chill$name == "Opt3")]),
          max(all_chill$value[which(all_chill$name == "Opt4")])),
        1)
chill_heat_eval[,"Growing Degree Hours"] <-
  round(c(max(all_heat$value[which(all_heat$name == "Obs")]),
          max(all_heat$value[which(all_heat$name == "Opt1")]),
          max(all_heat$value[which(all_heat$name == "Opt2")]),
          max(all_heat$value[which(all_heat$name == "Opt3")]),
          max(all_heat$value[which(all_heat$name == "Opt4")])),
        0)

knitr::kable(chill_heat_eval,row.names = FALSE)  %>%
  kable_styling("striped", position = "left",font_size = 10)
```

This comparison shows that the choice of interpolation method can have substantial impact on our impression of accumulated chill and heat. The `interpolate_gaps_hourly()` function in `chillR` outperformed all other methods evaluated here.

## `Exercises` on filling gaps {-#exercises_patching}

Please document all results of the following assignments in your `learning logbook`.

- You already downloaded some weather data in the exercises for the [Getting temperatures lesson](#get_temp_data). You can keep working with this.

1) Use `chillR` functions to find out how many gaps you have in this dataset (even if you have none, please still follow all further steps)
2) Create a list of the 25 closest weather stations using the `handle_gsod` function
3) Identify suitable weather stations for patching gaps
4) Download weather data for promising stations, convert them to `chillR` format and compile them in a list
5) Use the `patch_daily_temperatures` function to fill gaps
6) Investigate the results - have all gaps been filled?
7) If necessary, repeat until you have a dataset you can work with in further analyses
